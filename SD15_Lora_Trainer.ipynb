{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laure-m/LoRA/blob/main/SD15_Lora_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA trainer SD 1.5 - Stable Diffusion\n"
      ],
      "metadata": {
        "id": "pAVpv903OEoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font size=\"5\" color=\"orange\"> [CELL 00]\n",
        "!pip install --upgrade huggingface-hub==0.25.0"
      ],
      "metadata": {
        "id": "5B5tp6cwciRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font size=\"5\" color=\"orange\"> [CELL 01] </font>\n",
        "#@markdown Use a different `Project_folder` each time when you upload a dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "Project_folder = 'AI_PICS/training/GenRen' #@param {type:\"string\"}\n",
        "pretained_model_name = 'alpacaml/stable-diffusion-v1-5' #@param {type:\"string\"}\n",
        "Image_repeats = 75 #@param {type:\"integer\"}\n",
        "Number_of_epoches = 1 #@param {type:\"integer\"}\n",
        "Learning_rate = 0.0001 #@param {type:\"number\"}\n",
        "Triggering_keyword = 'architectural collage' #@param {type:\"string\"}\n",
        "Lora_name = 'style01' #@param {type:\"string\"}\n",
        "Lora_output_path = 'AI_PICS/Lora' #@param {type:\"string\"}\n",
        "Skip_image_upload = True #@param {type:\"boolean\"}\n",
        "#@markdown Remember to enable this after you have edited your captions.\n",
        "\n",
        "# construct paths\n",
        "projectPath = '/content/drive/MyDrive/' + Project_folder\n",
        "imagePath = projectPath + '/' + str(Image_repeats) + '_'+ Lora_name\n",
        "loraPath = '/content/drive/MyDrive/' + Lora_output_path\n",
        "\n",
        "!mkdir -p {loraPath}\n",
        "\n",
        "\n",
        "def install():\n",
        "  !pip list | grep bitsandbytes > /content/bitsandbytespip.txt\n",
        "  with open('/content/bitsandbytespip.txt', 'r') as file:\n",
        "      if 'bitsandbytes' in file.read():\n",
        "        print('Already installed.')\n",
        "        %cd /content/kohya_ss/\n",
        "        return\n",
        "\n",
        "  print('Installing...')\n",
        "\n",
        "  # Install requirements\n",
        "  !pip install dadaptation==3.1 diffusers[torch]==0.24.0 easygui==0.98.3 einops==0.6.0 fairscale==0.4.13 ftfy==6.1.1 gradio==3.36.1 huggingface-hub==0.24.1 #was 0.19.4\n",
        "  !pip install lion-pytorch==0.0.6 lycoris_lora==1.8.0.dev6 open-clip-torch==2.20.0 prodigyopt==1.0 pytorch-lightning==1.9.0 safetensors==0.3.1 timm==0.6.12\n",
        "  !pip install tk==0.1.0 transformers==4.30.2 voluptuous==0.13.1 wandb==0.15.0 xformers==0.0.20 omegaconf\n",
        "\n",
        "\n",
        "  # Install bitsandbytes\n",
        "  !git clone -b 0.41.0 https://github.com/TimDettmers/bitsandbytes\n",
        "  %cd /content/bitsandbytes\n",
        "  !CUDA_VERSION=118 make cuda11x\n",
        "  !python setup.py install\n",
        "\n",
        "  # Install kohya\n",
        "  %cd /content\n",
        "  !git clone https://github.com/bmaltais/kohya_ss.git\n",
        "  %cd kohya_ss/\n",
        "  !git checkout v21.8.9\n",
        "\n",
        "  # update torchvision to a compatible version\n",
        "  !pip install torch==2.0.1+cu117 torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "  # add pwd to python path or else blip captioning won't work\n",
        "  %env PYTHONPATH=/env/python:/content/kohya_ss\n",
        "\n",
        "\n",
        "if not Skip_image_upload:\n",
        "  # upload images\n",
        "  import os\n",
        "  from google.colab import files\n",
        "  import shutil\n",
        "  import torch\n",
        "  if os.path.exists(projectPath):\n",
        "    raise Exception(f'Error: Project folder {Project_folder} already exists. Please use a different project folder name. \\n')\n",
        "  else:\n",
        "    !mkdir -p {imagePath}\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = imagePath + '/' + filename\n",
        "        shutil.move(filename, dst_path)\n",
        "    print('Images uploaded successfully.\\n')\n",
        "\n",
        "\n",
        "\n",
        "install()\n",
        "\n",
        "# auto-captioning\n",
        "if not Skip_image_upload:\n",
        "  !python3 \"finetune/make_captions.py\" --batch_size=\"1\" --num_beams=\"1\"\\\n",
        "                          --top_p=\"0.9\" --max_length=\"75\" --min_length=\"20\" --beam_search\\\n",
        "                          --caption_extension=\".txt\"\\\n",
        "                          {imagePath}\\\n",
        "                          --caption_weights=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\"\n",
        "  # Add preflix to captions\n",
        "  import glob\n",
        "  for file in glob.glob(imagePath + \"/*.txt\"):\n",
        "    !echo {Triggering_keyword}, `cat {file}` > {file}\n",
        "    !cat {file}\n",
        "\n",
        "# Run training\n",
        "!accelerate launch --num_cpu_threads_per_process=2 \"./train_network.py\"     \\\n",
        "                         --enable_bucket --min_bucket_reso=256 --max_bucket_reso=2048               \\\n",
        "                         --pretrained_model_name_or_path={pretained_model_name}           \\\n",
        "                         --train_data_dir={projectPath}         \\\n",
        "                         --resolution=\"512,650\" --output_dir={loraPath}  \\\n",
        "                         --network_alpha=\"64\" --save_model_as=safetensors                           \\\n",
        "                         --network_module=networks.lora --text_encoder_lr=5e-05 --unet_lr={Learning_rate}    \\\n",
        "                         --network_dim=64 --output_name={Lora_name} --lr_scheduler_num_cycles=\"1\"        \\\n",
        "                         --no_half_vae --learning_rate={Learning_rate} --lr_scheduler=\"constant\"           \\\n",
        "                         --train_batch_size=\"3\" --max_train_steps=\"100000\" --save_every_n_epochs=\"99999\"   \\\n",
        "                         --mixed_precision=\"fp16\" --save_precision=\"fp16\" --seed=\"1234\"             \\\n",
        "                         --caption_extension=\".txt\" --cache_latents --optimizer_type=\"AdamW\"        \\\n",
        "                         --max_data_loader_n_workers=\"1\" --clip_skip=2 --bucket_reso_steps=64       \\\n",
        "                         --max_train_epochs={Number_of_epoches}\\\n",
        "                         --mem_eff_attn --xformers --bucket_no_upscale --noise_offset=0.05"
      ],
      "metadata": {
        "id": "NHsC6v8oxnyk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\" color=\"orange\"> Check / Edit Your Dataset </font>\n",
        "\n",
        "Change and edit the text files before re-running `CELL 01` with the `Skip_image_upload` button enabled to train your LoRA model."
      ],
      "metadata": {
        "id": "ytElEzn14YRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font size=\"5\" color=\"orange\"> [CELL 02] Setup Environment WITHOUT uploading dataset or training LoRA  </font>\n",
        "#@markdown Only run this cell if you already uploaded and edited dataset and trained your LoRA and just need to setup the environment in order to test generate some images.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "Project_folder = 'AI_PICS/training/GenRen' #@param {type:\"string\"}\n",
        "pretained_model_name = 'alpacaml/stable-diffusion-v1-5' #@param {type:\"string\"}\n",
        "Image_repeats = 100\n",
        "Number_of_epoches = 1\n",
        "Learning_rate = 0.0001\n",
        "Triggering_keyword = 'fashion'\n",
        "Lora_name = 'style01' #@param {type:\"string\"}\n",
        "Lora_output_path = 'AI_PICS/Lora' #@param {type:\"string\"}\n",
        "Skip_image_upload = True\n",
        "#@markdown These inputs should reference your already upoloaded and edited dataset.\n",
        "\n",
        "# construct paths\n",
        "projectPath = '/content/drive/MyDrive/' + Project_folder\n",
        "imagePath = projectPath + '/' + str(Image_repeats) + '_'+ Lora_name\n",
        "loraPath = '/content/drive/MyDrive/' + Lora_output_path\n",
        "\n",
        "!mkdir -p {loraPath}\n",
        "\n",
        "\n",
        "def install():\n",
        "  !pip list | grep bitsandbytes > /content/bitsandbytespip.txt\n",
        "  with open('/content/bitsandbytespip.txt', 'r') as file:\n",
        "      if 'bitsandbytes' in file.read():\n",
        "        print('Already installed.')\n",
        "        %cd /content/kohya_ss/\n",
        "        return\n",
        "\n",
        "  print('Installing...')\n",
        "\n",
        "  # Install requirements\n",
        "  !pip install dadaptation==3.1 diffusers[torch]==0.24.0 easygui==0.98.3 einops==0.6.0 fairscale==0.4.13 ftfy==6.1.1 gradio==3.36.1 huggingface-hub==0.19.4\n",
        "  !pip install lion-pytorch==0.0.6 lycoris_lora==1.8.0.dev6 open-clip-torch==2.20.0 prodigyopt==1.0 pytorch-lightning==1.9.0 safetensors==0.3.1 timm==0.6.12\n",
        "  !pip install tk==0.1.0 transformers==4.30.2 voluptuous==0.13.1 wandb==0.15.0 xformers==0.0.20 omegaconf\n",
        "\n",
        "\n",
        "  # Install bitsandbytes\n",
        "  !git clone -b 0.41.0 https://github.com/TimDettmers/bitsandbytes\n",
        "  %cd /content/bitsandbytes\n",
        "  !CUDA_VERSION=118 make cuda11x\n",
        "  !python setup.py install\n",
        "\n",
        "  # Install kohya\n",
        "  %cd /content\n",
        "  !git clone https://github.com/bmaltais/kohya_ss.git\n",
        "  %cd kohya_ss/\n",
        "  !git checkout v21.8.9\n",
        "\n",
        "  # update torchvision to a compatible version\n",
        "  !pip install torch==2.0.1+cu117 torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "  # add pwd to python path or else blip captioning won't work\n",
        "  %env PYTHONPATH=/env/python:/content/kohya_ss\n",
        "\n",
        "install()\n"
      ],
      "metadata": {
        "id": "5ugRlN_ezc4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font size=\"5\" color=\"orange\"> [CELL 03] Test Image Generation Using Your LoRA </font>\n",
        "#@markdown Write different prompts abd generate multiple versions to test out your LoRA\n",
        "#@title Test image generation from LoRA\n",
        "\n",
        "prompt = \"photo of runway model wearing a sculptural black dress with large colorful shoes and a red train\" #@param {type:\"string\"}\n",
        "negative_prompt = \"disfigured, deformed\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 25 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "seed = 100 #@param {type:\"number\"}\n",
        "\n",
        "%cd /content/\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
        "from matplotlib.pyplot import figure, imshow, axis\n",
        "from matplotlib.image import imread\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "if 'pipe' not in locals():\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(pretained_model_name, safety_checker=None, torch_dtype=torch.float16)\n",
        "  pipe.load_lora_weights(loraPath+'/'+Lora_name + '.safetensors')\n",
        "  pipe.to(\"cuda\")\n",
        "  pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "  g_cuda = None\n",
        "\n",
        "\n",
        "\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "\n",
        "g_cuda.manual_seed(seed)\n",
        "\n",
        "from torch import autocast\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "    from ipywidgets import widgets, HBox\n",
        "    from IPython.display import display\n",
        "    for im in images:\n",
        "        display(im)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MHrfhTLjJbmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\" color=\"orange\"> Re-Run CELL 03 with other prompts</font>\n",
        "\n",
        "Remember you can always edit, add to, or subtract from your dataset and upload to a new folder to re-train."
      ],
      "metadata": {
        "id": "ig0WcKrc64kU"
      }
    }
  ]
}